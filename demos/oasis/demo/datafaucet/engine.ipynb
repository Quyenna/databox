{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the engine\n",
    "\n",
    "Super simple, yet flexible :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n",
      "Connecting to spark master: local[*]\n",
      "Engine context spark:2.4.4 successfully started\n"
     ]
    }
   ],
   "source": [
    "#start the engine\n",
    "engine = dfc.engine('spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can also use directlt the specific engine class\n",
    "engine = dfc.SparkEngine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and saving data resources is an operation performed by the engine. The engine configuration can be passed straight as parameters in the engine call, or configured in metadata yaml files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine Context\n",
    "\n",
    "You can access the underlying engine by referring to the engine.context. In particular for the spark engine the context can be accessed with the next example code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = dfc.context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://8ddc3eb203a6:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>None</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f83d5b9ff60>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "|  a|  b|\n",
      "+---+---+\n",
      "|yes|  1|\n",
      "| no|  0|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dataframe with two columns, named resp. 'a' and 'b'\n",
    "\n",
    "df = spark.createDataFrame([('yes',1),('no',0)], ('a', 'b'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Engine configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = dfc.engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.driver.port': '45125',\n",
       " 'spark.app.id': 'local-1571294265734',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.master': 'local[*]',\n",
       " 'spark.executor.id': 'driver',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.ui.showConsoleProgress': 'true',\n",
       " 'spark.driver.host': '8ddc3eb203a6'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SPARK_HOME: /opt/spark\n",
       "HADOOP_HOME: /opt/hadoop\n",
       "JAVA_HOME: /usr/lib/jvm/java-8-openjdk-amd64\n",
       "PYSPARK_PYTHON: /opt/conda/bin/python\n",
       "PYSPARK_DRIVER_PYTHON: /opt/conda/bin/python\n",
       "PYTHONPATH: /opt/spark/python:/opt/spark/python/lib/py4j-0.10.7-src.zip\n",
       "PYSPARK_SUBMIT_ARGS: ' pyspark-shell'\n",
       "SPARK_DIST_CLASSPATH:"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the full configuration, please uncomment and execute the following statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'python_version': '3.6.8',\n",
       " 'hadoop_version': '3.2.1',\n",
       " 'hadoop_detect': 'spark',\n",
       " 'hadoop_home': '/opt/hadoop',\n",
       " 'spark_home': '/opt/spark',\n",
       " 'spark_classpath': ['/opt/spark/jars/*',\n",
       "  '/opt/hadoop/etc/hadoop',\n",
       "  '/opt/hadoop/share/hadoop/common/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/common/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/hdfs/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/mapreduce/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn',\n",
       "  '/opt/hadoop/share/hadoop/yarn/lib/*',\n",
       "  '/opt/hadoop/share/hadoop/yarn/*'],\n",
       " 'spark_classpath_source': '/opt/spark/conf/spark-env.sh'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting engine parameters during engine initalization\n",
    "\n",
    "Submit master, configuration parameters and services as engine params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init engine \"spark\"\n",
      "Configuring packages:\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "Connecting to spark master: spark://spark-master:7077\n",
      "Engine context spark:2.4.4 successfully started\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datafaucet.spark.engine.SparkEngine at 0x7f83d5d2af28>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datafaucet as dfc\n",
    "dfc.engine('spark', master='spark://spark-master:7077', services='postgres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spark.executor.id': 'driver',\n",
       " 'spark.driver.host': '8ddc3eb203a6',\n",
       " 'spark.app.id': 'app-20191017063810-0000',\n",
       " 'spark.submit.pyFiles': '/home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.rdd.compress': 'True',\n",
       " 'spark.master': 'spark://spark-master:7077',\n",
       " 'spark.repl.local.jars': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.serializer.objectStreamReset': '100',\n",
       " 'spark.driver.port': '35583',\n",
       " 'spark.submit.deployMode': 'client',\n",
       " 'spark.app.name': 'None',\n",
       " 'spark.files': 'file:///home/jovyan/.ivy2/jars/org.postgresql_postgresql-42.2.5.jar',\n",
       " 'spark.ui.showConsoleProgress': 'true'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.engine().conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
