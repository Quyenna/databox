{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datafaucet\n",
    "\n",
    "Datafaucet is a productivity framework for ETL, ML application. Simplifying some of the common activities which are typical in Data pipeline such as project scaffolding, data ingesting, start schema generation, forecasting etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datafaucet as dfc\n",
    "from datafaucet import logging as log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging\n",
    "\n",
    "One of the main things here is to have configuration and code separated in different files. Project is all about setting the correct working directories where to run and find your notebooks, python files and configuration files. When datafaucet project is loaded, it starts by searching for a `__main__.py` file, according to python module file naming conventions. When such a file is found, the corresponding directory is set as the root path for the project. All modules and alias paths are all relative to the project root path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "Logging can be configured via metadata.yml file. The logging section of the metadata will allow you to define three types of handlers: a stdout handler, a file handler, and a kafka handler. Here below the configuration details:\n",
    "\n",
    "```\n",
    "loggers:\n",
    "    root:\n",
    "        severity: info\n",
    "\n",
    "    datafaucet:\n",
    "        name: dfc\n",
    "        stdio:\n",
    "            enable: true\n",
    "            severity: notice\n",
    "        file:\n",
    "            enable: true\n",
    "            severity: notice\n",
    "        kafka:\n",
    "            enable: false\n",
    "            severity: info\n",
    "            hosts:\n",
    "                kafka-node1:9092\n",
    "                kafka-node2:9092\n",
    "            topic: dfc\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logs\n",
    "\n",
    "Logging via datafaucet support 5 levels:\n",
    "  - info\n",
    "  - notice\n",
    "  - warning\n",
    "  - error\n",
    "  - fatal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No project metadata loaded.\n",
    "Logging will work without loading any metadata project configuration, but in this case it will use the default cofiguration of the python root logger. By default, `debug`, `info` and `notice` level are filtered out. To enable the full functionality, including logging to kafka and logging the custom logging information about the project (sessionid, username, etc) you must load a project first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "a warning message\n",
      "this is an error\n",
      "critical condition\n"
     ]
    }
   ],
   "source": [
    "log.debug('debug')\n",
    "log.info('notice')\n",
    "log.notice('jnotice')\n",
    "log.warning('a warning message')\n",
    "log.error('this is an error')\n",
    "log.critical('critical condition')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading a metadata profile\n",
    "If a logging configuration is loaded, then extra functionality will be available. In particular, logging will log datafaucet specific info, such as the session id, and data can be passed as a dictionary, optionally with a custom message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created SparkEngine\n",
      "Init engine \"spark\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "could not autodetect driver to install for file, version None\n",
      "could not autodetect driver to install for hdfs, version 3.2.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring packages:\n",
      "  -  com.microsoft.sqlserver:mssql-jdbc:6.4.0.jre8\n",
      "  -  com.oracle.ojdbc:ojdbc8:12.2.0.1\n",
      "  -  mysql:mysql-connector-java:8.0.12\n",
      "  -  org.apache.hadoop:hadoop-aws:3.2.1\n",
      "  -  org.postgresql:postgresql:42.2.5\n",
      "  -  ru.yandex.clickhouse:clickhouse-jdbc:0.1.54\n",
      "Configuring conf:\n",
      "  -  spark.hadoop.fs.s3a.access.key : ****** (redacted)\n",
      "  -  spark.hadoop.fs.s3a.endpoint : http://minio:9000\n",
      "  -  spark.hadoop.fs.s3a.impl : org.apache.hadoop.fs.s3a.S3AFileSystem\n",
      "  -  spark.hadoop.fs.s3a.path.style.access : true\n",
      "  -  spark.hadoop.fs.s3a.secret.key : ****** (redacted)\n",
      "Connecting to spark master: local[*]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not start the engine context\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java gateway process exited before sending its port number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<datafaucet.project.Project at 0x7efc75776278>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfc.project.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE:dfc:run_code hello world\n"
     ]
    }
   ],
   "source": [
    "# custom message\n",
    "dfc.logging.notice('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:dfc:run_code data\n"
     ]
    }
   ],
   "source": [
    "# custom data\n",
    "dfc.logging.warning({'test_value':42})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:dfc:run_code custom message\n"
     ]
    }
   ],
   "source": [
    "# custom data and message\n",
    "dfc.logging.warning('custom message', extra={'more':123})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTICE:dfc:my_function data\n",
      "WARNING:dfc:my_nested_function another message\n",
      "ERROR:dfc:my_nested_function custom\n"
     ]
    }
   ],
   "source": [
    "# from a function\n",
    "\n",
    "def my_nested_function():\n",
    "    log.warning('another message')\n",
    "    log.error('custom',extra=[1,2,3])\n",
    "    \n",
    "def my_function():\n",
    "    log.notice({'a':'text', 'b':2})\n",
    "    my_nested_function()\n",
    "    \n",
    "my_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
